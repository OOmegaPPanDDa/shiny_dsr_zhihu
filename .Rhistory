# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/female.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 取得 情感分數
# Add sentiment score
data$senti_score <- senti(data)
# 取得 tfidf 文本相似度
# Add tf_idf_score for similarity
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# 取得 文本分群 feature
# 有兩種分群一個是直接的 kmeans, 一個是 pca 降階後再 kmeans
# 取出來的 feature 可以把它想成該回答與各族群的接近度
# Add cluster score
ans_cluster <- by(data, data$question_title, get_cluster_feature)
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
library(readr)
train <- read_csv("~/Desktop/pecuD3_final/train.csv")
View(train)
unique(train$STOP_NAME)
stop_name<-unique(train$STOP_NAME)
paste(stop_name,'火車站', collapse = '')
paste0(stop_name,'火車站', collapse = '')
stop_df <- data.frame(stop_name = stop_name)
stop_df
stop_df <- stop_df %>% mutate(paste(stop_name,'火車站',collapse=''))
library(dplyr)
stop_df <- stop_df %>% mutate(paste(stop_name,'火車站',collapse=''))
stop_df
View(stop_df)
stop_df <- stop_df %>% mutate(stop_t_name = paste(stop_name,'火車站',collapse=''))
stop_df = stop_df[c(,1)]
stop_df = stop_df[c(1),]
stop_name
stop_df <- data.frame(stop_name = stop_name)
colnames(stop_df)
colnames(stop_df) <- '1'
stop_df <- stop_df %>% mutate(2 = paste(1,'火車站',collapse=''))
stop_df <- stop_df %>% mutate('2' = paste('1','火車站',collapse=''))
colnames(stop_df) = one
colnames(stop_df) = 'one'
stop_df <- stop_df %>% mutate(two = paste(one,'火車站',collapse=''))
stop_df <- stop_df %>% mutate(two = paste(stop_df$one,'火車站',collapse=''))
stop_name
for(name in stop_name){}
stop_name
stop_df <- data.frame(stop_name = stop_name)
for(i in 1:nrow(stop_df)){
stop_df$stop_name[i] = paste(stop_df$stop_name[i],' 火車站',collapse = '')
}
View(stop_df)
stop_df <- data.frame(stop_name = stop_name)
stop_df <- data.frame(sname = stop_name)
for(i in 1:nrow(stop_df)){
stop_df[i,sname] = paste(stop_df[i,sname],' 火車站',collapse = '')
}
for(i in 1:nrow(stop_df)){
stop_df[i,'sname'] = paste(stop_df[i,'sname'],' 火車站',collapse = '')
}
stop_name
to_add<-rep('火車站',length(stop_name))
to_add
paste(stop_name,to_add,collapse = '')
paste0(stop_name,to_add,collapse = '')
paste0(stop_name,to_add)
stop_name<-paste0(stop_name,to_add)
stop_name
write(stop_name,'stop_name.txt')
install.packages(library(googleVis))
install.packages('googleVis')
library(googleVis)
df=data.frame(country=c("US", "GB", "BR"),
val1=c(10,13,14),
val2=c(23,12,32))
Line <- gvisLineChart(df)
plot(Line)
Line
Bar <- gvisBarChart(df)
plot(Bar)
print(Bar, file="~/Desktop/AndrewGeoMap.js")
print(Bar, "char",file="~/Desktop/AndrewGeoMap.js")
print(Bar, "chart", file="~/Desktop/AndrewGeoMap.js")
source('~/.active-rstudio-document')
demo(googleVis)
require(datasets)
states
View(state.x77)
View(state.name)
state.name
library(ggmap) library(mapproj)
原文網址：https://read01.com/7Rdan.html
install.packages(ggmap)
install.packages('ggmap;)
install.packages('ggmap;)
;d
ssdsd(0)
install.packages('ggmap')
library(ggmpa)
library(ggmpap)
library(ggmap)
map <- get_map(location = 'Taiwan', zoom = 4)
ggmap(map)
library(mapproj)
ggmap(map)
install.packages('ggptoto')
install.packages('ggproto')
install.packages('ggmap')
install.packages("ggmap")
ggmap(map)
library(mapproj)
library(ggmap)
map <- get_map(location = 'Taiwan', zoom = 4)
ggmap(map)
map <- get_map(location = 'China', zoom = 4)
ggmap(map)
devtools::install_github("dkahle/ggmap")
devtools::install_github("hadley/ggplot2")
library(readr)
train_flow <- read_csv("~/D3/pecuD3_final/train_flow.csv")
View(train_flow)
library(dplyr)
train_simple <- subset(train,year==2005)
train_simple <- subset(train_flow,year==2005)
library(ggmap)
map <- get_map(location = "Taiwan", zoom = 8, language = "zh-TW", maptype = "roadmap")
ggmap(map, darken = c(0.5, "white")) +geom_point(aes(x = long, y = lat),
color = "red", data = train_simple)
is.na(train_simple)
sum(is.na(train_flow))
sum(is.nan(train_flow))
sum(is.na(train_flow))
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = FALSE)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = FALSE)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = FALSE)+scale_alpha(range = c(0, 0.3), guide = 'legend')
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "green", high = "red",guide = 'legend')+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(,data=train_simple,low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")
+scale_fill_gradient(data=train_simple,low = "#132B43", high = "#56B1F7", space = "Lab", na.value = "grey50",
guide = "legend")+scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)
+stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..))
ggmap(map) +
geom_point(aes(x = lon, y = lat), size = 2, col="red",data = train_simple, alpha = 0.6) +
geom_density2d(data = train_simple, aes(x = lon, y=lat), size = 0.3)
ggmap(map) +
geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 0.6) +
geom_density2d(data = train_simple, aes(x = long, y=lat), size = 0.3)
ggmap(map)
+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)+
stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_simple, alpha = 1)+
stat_density2d(data = train_simple, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
train_simple <- subset(train_flow, year==2014)
View(train_simple)
train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow))
train_simple %>% group(stop_name) %>% mutate(yearFLow = sum(flow))
train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()
train_use <- train[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()[,c('stop_name','yearFlow','lat','long')]
train_use <- train_simple %>% group_by(stop_name) %>% mutate(yearFLow = sum(flow)) %>% ungroup()
train_use <- train_use[,c('stop_name','yearFlow','lat','long')]
View(train_use)
train_use <- train_use[,c('stop_name','yearFLow','lat','long')]
colnames(train_use)[2] = 'yearFlow'
train_use <- unique(train_use)
train_use <- train_use %>% arrange(desc(yearFlow))
View(train_use)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_use, alpha = 1)+
stat_density2d(data = train_use, aes(x = long, y=lat,fill = ..level.., alpha = ..level..)
,size = 0.01, bins = 16, geom = "polygon")+
scale_fill_gradient(low = "green", high = "red",guide = 'legend')+
scale_alpha(range = c(0, 0.3), guide = F)
ggmap(map)+geom_point(aes(x = long, y = lat), size = 2, col="red",data = train_use, alpha = 1)
write.csv(train_use,'2014_train_year_flow.csv',row.names = F)
View(train_simple)
View(train_use)
shiny::runApp('D3/pecuD3_final/pecuD3_shiny_final')
runApp('D3/pecuD3_final/pecuD3_shiny_final')
shiny::runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
install.packages('extrafont')
library(extrafont)
runApp('pecuR/shinyProject')
fonts()
fonttable()
library(extrafont)
fonttable()
fonttable()
fonts()
font_import()
font_import()
fonts()
fonttable()
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
shiny::runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
runApp('pecuR/shinyProject')
setwd("~/dsR/dsr_zhihu/shiny_dsr_zhihu")
shiny::runApp()
runApp()
runApp()
setwd("~/dsR/dsr_zhihu/shiny_dsr_zhihu")
shiny::runApp()
