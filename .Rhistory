# Add tf_idf_score for similarity
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# 取得 文本分群 feature
# 有兩種分群一個是直接的 kmeans, 一個是 pca 降階後再 kmeans
# 取出來的 feature 可以把它想成該回答與各族群的接近度
# Add cluster score
ans_cluster <- by(data, data$question_title, get_cluster_feature)
ans_cluster <- do.call("rbind", ans_cluster)
data$km1 <- ans_cluster$km1
data$km2 <- ans_cluster$km2
data$km3 <- ans_cluster$km3
data$pc1 <- ans_cluster$pc1
data$pc2 <- ans_cluster$pc2
data$pc3 <- ans_cluster$pc3
# 取得文本品質分數
# 文本品質分數 = scale(贊同數 * log(回應時間))
# 回應時間越短，代表越多人看到你的回答，贊同數理應較高
# define quality score
data$true_quality_score = as.vector(scale(data$ans_upvote_num/(log(as.numeric(data$response_time))+1)))
# 依文本品質分數取 median 後，分好壞兩種品質
# Add answer quality
ans_quality <- by(data, data$question_title, get_ans_quality)
data$quality <- unlist(ans_quality)
data$quality <- as.factor(data$quality)
# Take away the vector column
# data$ans_seg_vec <- NULL
head(data,20)
# 選擇某一問題
# (可以在上邊先選好)
data <- subset(data, data$ans_upvote_num != 0)
qid = 1
one_question_data <- data[data$question_title == unique(data$question_title)[qid],]
# 查看 data colnames
colnames(one_question_data)
# 選擇 svm 參數
# select data
selected_data <- one_question_data[,c('author_follower_num','author_followee_num','author_upvote_num','author_thank_num','author_answer_num','author_question_num','author_post_num','n_char','n_word','n_stop','per_stop','senti_score','ans_tfidf','km1','km2','km3','pc1','pc2','pc3','quality')]
head(selected_data,20)
# 做 train_test_split
testset <- take_sample(selected_data)
trainset <- anti_join(selected_data,testset)
# traininig model
(tuned <- tune.svm(quality~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2), probability = TRUE))
model <- svm(quality~., data = trainset, cost = 1, gamma = 2, probability = TRUE)
# 看 model 預測效果
prediction <- predict(model, testset[,-ncol(testset)])
prediction
data.frame(prediction, testset$quality)
ConfusionMatrix(prediction, testset$quality)
# 用 f1 看 model 品質
F1_Score(prediction, testset$quality)
# save model
save(model, file = "svm_model.rda")
# save model
svm_model <- model
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count < 100)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 取得 情感分數
# Add sentiment score
data$senti_score <- senti(data)
# 取得 tfidf 文本相似度
# Add tf_idf_score for similarity
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# 取得 文本分群 feature
# 有兩種分群一個是直接的 kmeans, 一個是 pca 降階後再 kmeans
# 取出來的 feature 可以把它想成該回答與各族群的接近度
# Add cluster score
ans_cluster <- by(data, data$question_title, get_cluster_feature)
ans_cluster <- do.call("rbind", ans_cluster)
data$km1 <- ans_cluster$km1
data$km2 <- ans_cluster$km2
data$km3 <- ans_cluster$km3
data$pc1 <- ans_cluster$pc1
data$pc2 <- ans_cluster$pc2
data$pc3 <- ans_cluster$pc3
# 取得文本品質分數
# 文本品質分數 = scale(贊同數 * log(回應時間))
# 回應時間越短，代表越多人看到你的回答，贊同數理應較高
# define quality score
data$true_quality_score = as.vector(scale(data$ans_upvote_num/(log(as.numeric(data$response_time))+1)))
# 依文本品質分數取 median 後，分好壞兩種品質
# Add answer quality
ans_quality <- by(data, data$question_title, get_ans_quality)
data$quality <- unlist(ans_quality)
data$quality <- as.factor(data$quality)
# Take away the vector column
# data$ans_seg_vec <- NULL
head(data,20)
# 選擇某一問題
# (可以在上邊先選好)
data <- subset(data, data$ans_upvote_num != 0)
qid = 1
one_question_data <- data[data$question_title == unique(data$question_title)[qid],]
# 查看 data colnames
colnames(one_question_data)
# 選擇 svm 參數
# select data
selected_data <- one_question_data[,c('author_follower_num','author_followee_num','author_upvote_num','author_thank_num','author_answer_num','author_question_num','author_post_num','n_char','n_word','n_stop','per_stop','senti_score','ans_tfidf','km1','km2','km3','pc1','pc2','pc3','quality')]
head(selected_data,20)
# 做 train_test_split
testset <- take_sample(selected_data)
trainset <- anti_join(selected_data,testset)
# traininig model
(tuned <- tune.svm(quality~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2), probability = TRUE))
model <- svm(quality~., data = trainset, cost = 1, gamma = 2, probability = TRUE)
# 看 model 預測效果
prediction <- predict(model, testset[,-ncol(testset)])
prediction
data.frame(prediction, testset$quality)
ConfusionMatrix(prediction, testset$quality)
# 用 f1 看 model 品質
F1_Score(prediction, testset$quality)
# save model
svm_model <- model
svm_question_title <- one_question_data$question_title[1]
svm_question_detail <- one_question_data$question_detail[1]
save(svm_model, file = "svm_model.rda")
save(svm_question_title, file = "svm_question_title.rda")
save(svm_question_detail, file = "svm_question_detail.rda")
print(svm_question_title)
print(svm_question_detail)
print('Thank You~')
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/business.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 讀取全部會用到的套件
library(readr)
library(dplyr)
library(jiebaR)
library(tidytext)
library(fpc)
library(cluster)
library(rJava)
library(tm)
library(SnowballC)
library(slam)
library(XML)
library(RCurl)
library(Matrix)
library(tmcn)
library(Rwordseg)
library(e1071)
library(MLmetrics)
# 讀取寫好的 function file
source('zhihu_preprocessing.R')
source('zhihu_utility.R')
source('zhihu_senti.R')
source('zhihu_cluster.R')
source('zhihu_tfidf_score.R')
# 從 dataset 中擇ㄧ個話題的 csv 檔
# Global variables
data <- tbl_df(read_csv("./data_collect/female.csv")) %>% na.omit()
data <- as.data.frame(data)
# 排除未被按讚答案 ( ans_up_votenum == 0 )
data <- number_filter(data)
# 濾詞與濾掉空白和 NA 的 row
# clean_text and omit na
data <- text_filter(data)
#  塞選 answer 數在 100 筆以上的 question
data <- data %>%
group_by(question_title) %>%
mutate(ans_count = n()) %>%
ungroup() %>%
filter(ans_count > 400)
# 將 data 按 question_title 排列
# Reorder the data by question_title
data <- data[order(data$question_title),]
# 選擇某一問題
# (可以先 comment，到後面再選，只是程式負擔會比較大)
data <- subset(data, data$ans_upvote_num != 0)
# 隨機取一個問題
qid = sample(1:length(unique(data$question_title)),1)
#qid = 1
data <- data[data$question_title == unique(data$question_title)[qid],]
# 取得 stop_word
# Get stop words
# data$question_combined <- paste(data$question_title, data$question_detail)
# document <- c(unique(data$question_combined),unique(data$ans))
# stop_word <- get_stop_word(document)
# stop_word <- unique(c(stop_word, toTrad(stopwordsCN())))
stop_word <- readLines('all_stop_word.txt')
# 取得 回應時間
# Add response time
data$response_time <- time_transform(data)
# 取得 斷詞 vector
# 取得 斷詞後濾掉 stop_word 的 vector
# 取得 斷詞後濾掉 stop_word 的 character string
# Add segmented questions and answers
data$ans_seg_vec_with_stop <- sapply(data$ans, function(x) seg_worker[x])
data$ans_seg_vec <- sapply(data$ans_seg_vec_with_stop, function(x) filter_segment(x, stop_word))
data$ans_seg <- sapply(data$ans_seg_vec, function(x) paste(x, collapse = ' '))
#data$q_seg <- sapply(data$question_combined, function(x) paste(filter_segment(seg_worker[x], stop_word), collapse = ' '))
#取得字數、詞數、stop_word 數和 stop_word 比例
# Add number of characters, words, and percentage of stop words
data$n_char <- nchar(data$ans)
#data$n_word <- word_count(data)
data$n_word <- sapply(data$ans_seg_vec_with_stop, function(x) length(x))
data$n_stop <- sapply(data$ans_seg_vec_with_stop, function(x) sum(is.element(x, stop_word)))
data$per_stop <- data$n_stop/data$n_word
# 取得 情感分數
# Add sentiment score
data$senti_score <- senti(data)
# 取得 tfidf 文本相似度
# Add tf_idf_score for similarity
ans_tfidf <- by(data, data$question_title, tf_idf_score)
data$ans_tfidf <- unlist(ans_tfidf)
# 取得 文本分群 feature
# 有兩種分群一個是直接的 kmeans, 一個是 pca 降階後再 kmeans
# 取出來的 feature 可以把它想成該回答與各族群的接近度
# Add cluster score
ans_cluster <- by(data, data$question_title, get_cluster_feature)
shiny::runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
runApp('dsR/dsr_zhihu/shiny_dsr_zhihu')
setwd("~/dsR/dsr_zhihu/shiny_dsr_zhihu")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
